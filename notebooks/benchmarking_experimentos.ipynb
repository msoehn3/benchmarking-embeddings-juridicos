{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoxxqC_FDzwN"
      },
      "source": [
        "# Preparação do Ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5o_BZ4a00R_"
      },
      "source": [
        "## Instalação dos pacotes necessários"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCsn7P3twY1W"
      },
      "outputs": [],
      "source": [
        "!pip install nltk gensim spacy sentence-transformers scikit-learn seaborn matplotlib pandas hdbscan\n",
        "!python -m spacy download pt_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fkb9Yt_1BCb"
      },
      "source": [
        "## Importação das bibliotecas\n",
        "Após a instalação, tem que reiniciar a sessão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BpQN56kzj6L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import zipfile\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from IPython.display import display\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "\n",
        "from gensim.models import FastText, Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import CoherenceModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import umap\n",
        "\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "from scipy.spatial.distance import cdist\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-VoOdWvA1as"
      },
      "source": [
        "# Definição de Funções Úteis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahml3XdLExDi"
      },
      "source": [
        "## Carregar corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxz97r65E0-x"
      },
      "outputs": [],
      "source": [
        "def carregar_corpus(path, qtde_max = 999999999):\n",
        "    ids = []\n",
        "    ementas = []\n",
        "\n",
        "    arquivos = [f for f in os.listdir(path) if f.endswith('.txt')]\n",
        "\n",
        "    cont = 0\n",
        "    for arquivo in arquivos:\n",
        "        if cont == qtde_max:\n",
        "            break\n",
        "        cont += 1\n",
        "\n",
        "        id_acordao = os.path.splitext(arquivo)[0]  # remove .txt\n",
        "        with open(os.path.join(path, arquivo), 'r', encoding='utf-8') as f:\n",
        "            texto = f.read().strip()\n",
        "\n",
        "        # Remove a citação final entre parênteses, se houver\n",
        "        texto_limpo = re.sub(r'\\s*\\(TRT.*\\)$', '', texto)\n",
        "\n",
        "        ids.append(id_acordao)\n",
        "        ementas.append(texto_limpo)\n",
        "\n",
        "    df = pd.DataFrame({'id_acordao': ids, 'ementa': ementas})\n",
        "    df['ementa'] = df['ementa'].astype(str)\n",
        "\n",
        "    print(f\"Total de ementas carregadas: {len(df)}\")\n",
        "    print(df.sample(5))\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GxdeukVmpCE"
      },
      "source": [
        "## Contar palavras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4QpPjHxmoIu"
      },
      "outputs": [],
      "source": [
        "def contar_palavras(texto):\n",
        "    if isinstance(texto, list):\n",
        "        return len(texto)\n",
        "    elif isinstance(texto, str):\n",
        "        return len(texto.split())\n",
        "    else:\n",
        "        return 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T9NiOs3-4Ag"
      },
      "source": [
        "## Contar caracteres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMHW-Jqp-6OR"
      },
      "outputs": [],
      "source": [
        "def contar_caracteres(texto):\n",
        "    if isinstance(texto, list):\n",
        "        return sum(len(p) for p in texto)\n",
        "    elif isinstance(texto, str):\n",
        "        return len(texto)\n",
        "    else:\n",
        "        return 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCVI0eNCYLVW"
      },
      "source": [
        "## Lematizar texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2f-HUIcYNw8"
      },
      "outputs": [],
      "source": [
        "def lematizar_texto(texto):\n",
        "    doc = nlp_spacy(texto)\n",
        "    lemas = [\n",
        "        token.lemma_\n",
        "        for token in doc\n",
        "        if token.is_alpha and not token.is_stop\n",
        "    ]\n",
        "    return lemas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLAI3KNNBh8M"
      },
      "source": [
        "## Remover stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjdyFDSWBlc4"
      },
      "outputs": [],
      "source": [
        "def remover_stopwords(text, stopwords_remover):\n",
        "\n",
        "    if isinstance(text, list):\n",
        "        text = ' '.join(text)  # transforma lista em string se for o caso\n",
        "\n",
        "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
        "    text = re.sub(r\"\\d+\", \" \", text)\n",
        "    tokens = word_tokenize(text.lower(), language=\"portuguese\")\n",
        "\n",
        "    tokens = [\n",
        "        token\n",
        "        for token in tokens\n",
        "        if token not in stopwords_remover and len(token) > 2 and token.isalpha()\n",
        "    ]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpHLNUMeDOZK"
      },
      "source": [
        "## Carregar palavras mais frequentes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmq54irhYeAg"
      },
      "outputs": [],
      "source": [
        "def carregar_palavras_mais_frequentes(lista_ementa, stopwords_ignorar, top_n=20):\n",
        "\n",
        "    # Junta todas as ementas em um único texto\n",
        "    texto_unificado = ' '.join([str(e) for e in lista_ementa])\n",
        "\n",
        "    # Limpeza básica\n",
        "    texto_unificado = re.sub(r\"[^\\w\\s]\", \" \", texto_unificado)  # remove pontuação\n",
        "    texto_unificado = re.sub(r\"\\d+\", \" \", texto_unificado)      # remove números\n",
        "    texto_unificado = texto_unificado.lower()                   # minúsculas\n",
        "\n",
        "    # Tokenização\n",
        "    tokens = word_tokenize(texto_unificado, language='portuguese')\n",
        "\n",
        "    # Remove stopwords e tokens não alfabéticos\n",
        "    tokens_filtrados = [\n",
        "        token for token in tokens\n",
        "        if token not in stopwords_ignorar and len(token) > 2 and token.isalpha()\n",
        "    ]\n",
        "\n",
        "    # Contagem das palavras\n",
        "    counter = Counter(tokens_filtrados)\n",
        "    most_common = counter.most_common(top_n)\n",
        "\n",
        "    # Separa palavras e frequências\n",
        "    palavras, contagens = zip(*most_common) if most_common else ([], [])\n",
        "\n",
        "    return palavras, contagens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTDbTXlEBZSI"
      },
      "source": [
        "## Gerar Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcNNQ0ZtBb1j"
      },
      "outputs": [],
      "source": [
        "def gerar_embeddings(corpus, modelo='sbert'):\n",
        "\n",
        "    corpus_texto = [' '.join(doc) if isinstance(doc, list) else doc for doc in corpus]\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    if modelo == 'fasttext':\n",
        "        tokenizado = [doc.split() for doc in corpus_texto]\n",
        "        ft_model = FastText(sentences=tokenizado, vector_size=300, window=5, min_count=2, workers=4, epochs=20)\n",
        "        for doc in tokenizado:\n",
        "            vetor = np.mean(\n",
        "                [ft_model.wv[word] for word in doc if word in ft_model.wv] or [np.zeros(300)],\n",
        "                axis=0\n",
        "            )\n",
        "            embeddings.append(vetor)\n",
        "\n",
        "    elif modelo == 'doc2vec':\n",
        "        tokenizado = [doc.split() for doc in corpus_texto]\n",
        "        documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokenizado)]\n",
        "        d2v_model = Doc2Vec(documents, vector_size=300, window=5, min_count=2, workers=4, epochs=40)\n",
        "        for doc in tokenizado:\n",
        "            if len(doc) == 0:\n",
        "                vetor = np.zeros(300)\n",
        "            else:\n",
        "                vetor = d2v_model.infer_vector(doc)\n",
        "            embeddings.append(vetor)\n",
        "\n",
        "    elif modelo == 'sbert':\n",
        "        sbert = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "        embeddings = sbert.encode(corpus_texto, show_progress_bar=True)\n",
        "\n",
        "    elif modelo == 'bertimbau':\n",
        "        sbert = SentenceTransformer('neuralmind/bert-base-portuguese-cased')\n",
        "        embeddings = sbert.encode(corpus_texto, show_progress_bar=True)\n",
        "\n",
        "    elif modelo == 'legalbert':\n",
        "        sbert = SentenceTransformer('raquelsilveira/legalbertpt_fp')\n",
        "        embeddings = sbert.encode(corpus_texto, show_progress_bar=True)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Modelo não reconhecido. Escolha entre 'fasttext', 'doc2vec', 'sbert', 'bertimbau' ou 'legalbert'.\")\n",
        "\n",
        "    embeddings = np.vstack(embeddings) if isinstance(embeddings, list) else np.array(embeddings)\n",
        "    return embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPKWGGF4kEzj"
      },
      "source": [
        "## Reduzir dimensionalidade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yleCdcqkH2d"
      },
      "outputs": [],
      "source": [
        "def reduzir_dimensionalidade(embeddings, n_components=2):\n",
        "    reducer = umap.UMAP(n_components=n_components, random_state=SEED)\n",
        "    reduzido = reducer.fit_transform(embeddings)\n",
        "    return reduzido\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBSqnwUukx7B"
      },
      "source": [
        "## Aplicar clusterização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgTxgroNk0W_"
      },
      "outputs": [],
      "source": [
        "def aplicar_clusterizacao(embeddings, min_cluster_size=5):\n",
        "    modelo = HDBSCAN(\n",
        "        min_cluster_size=min_cluster_size,\n",
        "        metric='euclidean',\n",
        "        cluster_selection_method='eom'\n",
        "    ).fit(embeddings)\n",
        "    labels = modelo.labels_\n",
        "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "    print(f\"🔸 HDBSCAN encontrou {n_clusters} clusters (sem contar ruído)\")\n",
        "\n",
        "    return labels, modelo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmY53e6Xk6sM"
      },
      "source": [
        "## Extrair termos relevantes cluster (tf-idf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GHJ3NQ3k--t"
      },
      "outputs": [],
      "source": [
        "def termos_por_cluster(textos, labels, top_n=10):\n",
        "\n",
        "    textos_str = [' '.join(doc) if isinstance(doc, list) else doc for doc in textos]\n",
        "\n",
        "    df = pd.DataFrame({'texto': textos_str, 'cluster': labels})\n",
        "\n",
        "    resultados = {}\n",
        "    for cluster in sorted(df['cluster'].unique()):\n",
        "        docs = df[df['cluster'] == cluster]['texto']\n",
        "        vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "        X = vectorizer.fit_transform(docs)\n",
        "        indices = np.argsort(X.toarray().sum(axis=0))[::-1]\n",
        "        termos = [vectorizer.get_feature_names_out()[i] for i in indices[:top_n]]\n",
        "        resultados[cluster] = termos\n",
        "\n",
        "    return resultados\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stYxZS4tlGUh"
      },
      "source": [
        "## Avaliar clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQtI6dWAlKzn"
      },
      "outputs": [],
      "source": [
        "def avaliar_clusters(embeddings, labels, modelo):\n",
        "    mask = labels != -1  # Ignorar ruído para HDBSCAN\n",
        "\n",
        "    if len(set(labels[mask])) <= 1:\n",
        "        print(\"⚠️ Apenas um cluster detectado (ou ruído excessivo). Avaliação não aplicável.\")\n",
        "        return {\n",
        "            'Silhueta': np.nan,\n",
        "            'Calinski-Harabasz': np.nan\n",
        "        }\n",
        "\n",
        "    resultados = {}\n",
        "\n",
        "    resultados['Silhueta'] = silhouette_score(embeddings[mask], labels[mask])\n",
        "    resultados['Calinski-Harabasz'] = calinski_harabasz_score(embeddings[mask], labels[mask])\n",
        "\n",
        "    return resultados\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jakZCMHBhqL"
      },
      "source": [
        "## Calcular coherence score (c_v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOVAgPzfBlko"
      },
      "outputs": [],
      "source": [
        "def calcular_coherence(textos, labels, top_n=10):\n",
        "\n",
        "    df = pd.DataFrame({'texto': textos, 'cluster': labels})\n",
        "    df = df[df['cluster'] != -1].reset_index(drop=True)  # Remove ruídos e reseta os índices\n",
        "\n",
        "    # Vetorizar todos os documentos\n",
        "    docs_str = df['texto'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "    tfidf = vectorizer.fit_transform(docs_str)\n",
        "    termos = vectorizer.get_feature_names_out()\n",
        "\n",
        "    topics = []\n",
        "\n",
        "    for cluster in sorted(df['cluster'].unique()):\n",
        "        docs_cluster = df[df['cluster'] == cluster]\n",
        "        indices = docs_cluster.index\n",
        "        submatriz = tfidf[indices]\n",
        "\n",
        "        if submatriz.shape[0] == 0:\n",
        "            continue\n",
        "\n",
        "        # Soma TF-IDF dos termos no cluster e seleciona os top_n\n",
        "        tfidf_soma = np.asarray(submatriz.sum(axis=0)).flatten()\n",
        "        top_indices = tfidf_soma.argsort()[-top_n:][::-1]\n",
        "        top_termos = [termos[i] for i in top_indices]\n",
        "        topics.append(top_termos)\n",
        "\n",
        "    # Preparar corpus e dictionary para CoherenceModel\n",
        "    all_tokens = [doc.split() if isinstance(doc, str) else doc for doc in textos]\n",
        "    dictionary = Dictionary(all_tokens)\n",
        "\n",
        "    coherence_model = CoherenceModel(\n",
        "        topics=topics,\n",
        "        texts=all_tokens,\n",
        "        dictionary=dictionary,\n",
        "        coherence='c_v'\n",
        "    )\n",
        "\n",
        "    coherence = coherence_model.get_coherence()\n",
        "    print(f\"🔸 Coherence Score (TF-IDF): {coherence:.4f}\")\n",
        "    return coherence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GukAAxb2lVim"
      },
      "source": [
        "## Plotar clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHvk1WH5lXe3"
      },
      "outputs": [],
      "source": [
        "def plotar_clusters(embeddings_reduzidos, labels, titulo='Clusters'):\n",
        "\n",
        "    plt.figure(figsize=(10,7))\n",
        "    palette = sns.color_palette(\"tab10\", len(set(labels)))\n",
        "\n",
        "    sns.scatterplot(\n",
        "        x=embeddings_reduzidos[:, 0],\n",
        "        y=embeddings_reduzidos[:, 1],\n",
        "        hue=labels,\n",
        "        palette=palette,\n",
        "        legend='full',\n",
        "        alpha=0.7,\n",
        "        s=60,\n",
        "        edgecolor='black'\n",
        "    )\n",
        "\n",
        "    plt.title(titulo, fontsize=14)\n",
        "    plt.xlabel('Componente 1')\n",
        "    plt.ylabel('Componente 2')\n",
        "    plt.grid(visible=True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "    plt.legend(title='Cluster', loc='best')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riNJH2jqwmrc"
      },
      "source": [
        "## Pipeline avaliação embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UngReryUwtjq"
      },
      "outputs": [],
      "source": [
        "def pipeline_embedding(\n",
        "    nome_modelo, textos, n_components=2,\n",
        "    min_cluster_size=5\n",
        "):\n",
        "\n",
        "    print(f'\\n🔹 Processando modelo: {nome_modelo}')\n",
        "\n",
        "    inicio = time.time()\n",
        "    embeddings = gerar_embeddings(textos, modelo=nome_modelo)\n",
        "\n",
        "    reduzido = reduzir_dimensionalidade(embeddings, n_components=n_components)\n",
        "\n",
        "    melhor_k = None\n",
        "\n",
        "    labels, modelo_cluster = aplicar_clusterizacao(\n",
        "        embeddings, min_cluster_size=min_cluster_size\n",
        "    )\n",
        "\n",
        "    tempo_embedding = time.time() - inicio\n",
        "    print(f'⏳ Tempo Pipeline ({nome_modelo}): {tempo_embedding:.2f}s')\n",
        "\n",
        "    avaliacao = avaliar_clusters(embeddings, labels, modelo_cluster)\n",
        "    avaliacao['Modelo'] = nome_modelo\n",
        "    avaliacao['K'] = (len(set(labels)) - (1 if -1 in labels else 0))\n",
        "    avaliacao['Tempo Pipeline (s)'] = tempo_embedding\n",
        "    avaliacao['Coherence c_v'] = calcular_coherence(textos, labels)\n",
        "\n",
        "    return avaliacao, labels, embeddings, reduzido, avaliacao['K'], modelo_cluster\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni43QuG7w8LI"
      },
      "source": [
        "## Comparar modelos e coletar avaliações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW68Y3fcxBDb"
      },
      "outputs": [],
      "source": [
        "def comparar_modelos(\n",
        "    modelos, textos, n_components=2,\n",
        "    min_cluster_size=5\n",
        "):\n",
        "    resultados = []\n",
        "    embeddings_dict = {}\n",
        "    labels_dict = {}\n",
        "    reduzidos_dict = {}\n",
        "    k_dict = {}\n",
        "    modelos_cluster = {}\n",
        "\n",
        "    for modelo in modelos:\n",
        "        avaliacao, labels, embeddings, reduzido, k, modelo_cluster = pipeline_embedding(\n",
        "            modelo, textos, min_cluster_size=min_cluster_size,\n",
        "            n_components=n_components\n",
        "        )\n",
        "        resultados.append(avaliacao)\n",
        "        embeddings_dict[modelo] = embeddings\n",
        "        labels_dict[modelo] = labels\n",
        "        reduzidos_dict[modelo] = reduzido\n",
        "        k_dict[modelo] = k\n",
        "        modelos_cluster[modelo] = modelo_cluster\n",
        "\n",
        "    df_resultados = pd.DataFrame(resultados)\n",
        "    df_resultados.set_index('Modelo', inplace=True)\n",
        "\n",
        "    return df_resultados, embeddings_dict, labels_dict, reduzidos_dict, k_dict, modelos_cluster\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFtovvL_bwFn"
      },
      "source": [
        "## Visualizar termos do cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ByxTImdbzA8"
      },
      "outputs": [],
      "source": [
        "def visualizar_clusters_termos(textos, labels, modelo_nome, top_n=10, n_amostra=5):\n",
        "    df = pd.DataFrame({'texto': textos, 'cluster': labels})\n",
        "    clusters_unicos = [c for c in set(labels) if c != -1]\n",
        "    clusters_amostrados = np.random.choice(clusters_unicos, min(n_amostra, len(clusters_unicos)), replace=False)\n",
        "\n",
        "    print(f'\\n🟦 Modelo: {modelo_nome}')\n",
        "    print(f'Clusters amostrados: {clusters_amostrados}')\n",
        "\n",
        "    for cluster in clusters_amostrados:\n",
        "        docs = df[df['cluster'] == cluster]['texto']\n",
        "        docs_tokens = [doc if isinstance(doc, list) else doc.split() for doc in docs]\n",
        "        all_tokens = [token for doc in docs_tokens for token in doc]\n",
        "        contador = Counter(all_tokens)\n",
        "        termos_top = [t for t, _ in contador.most_common(top_n)]\n",
        "\n",
        "        print(f'\\n🔹 Cluster {cluster}:')\n",
        "        print(f'Termos mais representativos: {termos_top}')\n",
        "        print(f'Número de documentos no cluster: {len(docs)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDe8-Dd2-3fZ"
      },
      "source": [
        "# Execução"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjuJkPAJ_B-a"
      },
      "source": [
        "## Define variáveis e parâmetros gerais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDOONzSv_FrE"
      },
      "outputs": [],
      "source": [
        "# Sementes globais\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "QTDE_ACORDAOS_PROCESSADOS = 10000\n",
        "\n",
        "# palavras mais comuns\n",
        "TOP_N_WORDS = 55\n",
        "\n",
        "# Caminho do arquivo\n",
        "path = '/content/ementas'\n",
        "df = carregar_corpus(path, QTDE_ACORDAOS_PROCESSADOS)\n",
        "\n",
        "# Stopwords\n",
        "stopwords_pt = set(stopwords.words(\"portuguese\"))\n",
        "\n",
        "# Carrega o modelo de português\n",
        "nlp_spacy = spacy.load('pt_core_news_sm')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvCtH_h-OTNS"
      },
      "source": [
        "## 3. Análise Exploratória Antes do Pré-Processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdBPb9TO8NW9"
      },
      "source": [
        "### Amostra do corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz3zu5jh7_PP"
      },
      "outputs": [],
      "source": [
        "# Adiciona colunas de contagem\n",
        "df['qtde_palavras'] = df['ementa'].apply(contar_palavras)\n",
        "df['qtde_caractere'] = df['ementa'].apply(lambda x: len(str(x)))\n",
        "\n",
        "# Gera amostra com 5 ementas\n",
        "amostra = df[['id_acordao', 'ementa', 'qtde_palavras', 'qtde_caractere']].sample(5)\n",
        "\n",
        "display(amostra)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnGz7g_IPCAs"
      },
      "source": [
        "### Nuvem de palavras (sem pré-processamento)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_OU_KJgPDjv"
      },
      "outputs": [],
      "source": [
        "text_all = ' '.join(df['ementa'])\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text_all)\n",
        "\n",
        "plt.figure(figsize=(15,7))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Nuvem de Palavras (Antes do Pré-processamento)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qso-F6ZOa4E"
      },
      "source": [
        "### Tamanho dos textos (em caracteres e palavras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_0_giZbOSji"
      },
      "outputs": [],
      "source": [
        "df['n_caracteres'] = df['ementa'].apply(len)\n",
        "df['n_palavras'] = df['ementa'].apply(contar_palavras)\n",
        "\n",
        "print(df[['n_caracteres', 'n_palavras']].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6gonnhYOnPR"
      },
      "source": [
        "### Histograma do tamanho dos textos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Siv1QKYJOq-s"
      },
      "outputs": [],
      "source": [
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Calcula estatísticas\n",
        "media = df['n_palavras'].mean()\n",
        "mediana = df['n_palavras'].median()\n",
        "desvio = df['n_palavras'].std()\n",
        "\n",
        "plt.figure(figsize=(14,7))\n",
        "sns.histplot(df['n_palavras'], bins=50, kde=True, color='royalblue')\n",
        "\n",
        "# Linhas de média e mediana\n",
        "plt.axvline(media, color='red', linestyle='--', linewidth=2, label=f'Média: {media:.0f}')\n",
        "plt.axvline(mediana, color='green', linestyle='-', linewidth=2, label=f'Mediana: {mediana:.0f}')\n",
        "\n",
        "plt.title('Distribuição do Número de Palavras nas Ementas\\n(Antes do Pré-processamento)', fontsize=16)\n",
        "plt.xlabel('Número de Palavras por Ementa', fontsize=12)\n",
        "plt.ylabel('Frequência de Ementas', fontsize=12)\n",
        "\n",
        "plt.grid(visible=True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhikN8ZkRd1W"
      },
      "source": [
        "### Frequência das Palavras Mais Comuns (stopwords customizadas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRcmgtuMRghW"
      },
      "outputs": [],
      "source": [
        "df['ementa_lematizada'] = df['ementa'].apply(lematizar_texto)\n",
        "\n",
        "palavras, contagens = carregar_palavras_mais_frequentes(df['ementa_lematizada'], stopwords_pt, top_n = TOP_N_WORDS)\n",
        "plt.figure(figsize=(12,12))\n",
        "sns.barplot(x=list(contagens), y=list(palavras), palette='viridis')\n",
        "plt.title(f'Top {TOP_N_WORDS} Palavras Mais Frequentes (Após Pré-processamento)')\n",
        "plt.xlabel('Frequência')\n",
        "plt.ylabel('Palavras')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMJwFIKBUyIV"
      },
      "outputs": [],
      "source": [
        "def analisar_distribuicao_palavras(contador):\n",
        "    contagens_ordenadas = np.array(sorted(contador.values(), reverse=True))\n",
        "    soma_total = contagens_ordenadas.sum()\n",
        "    cumulativa = np.cumsum(contagens_ordenadas) / soma_total\n",
        "\n",
        "    pontos = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99]\n",
        "    resultados = {}\n",
        "\n",
        "    for p in pontos:\n",
        "        idx = np.searchsorted(cumulativa, p)\n",
        "        resultados[p] = idx + 1  # +1 porque index começa no zero\n",
        "\n",
        "    print(\"Palavras necessárias para atingir os percentuais acumulados:\")\n",
        "    for p, n in resultados.items():\n",
        "        print(f\"{int(p*100)}% -> {n} palavras\")\n",
        "\n",
        "    # Plotando a curva\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(range(1, len(cumulativa)+1), cumulativa, marker='.')\n",
        "    plt.title('Curva de Frequência Cumulativa')\n",
        "    plt.xlabel('Top N palavras')\n",
        "    plt.ylabel('Frequência acumulada')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Marcar os pontos de interesse\n",
        "    for p, n in resultados.items():\n",
        "        plt.axvline(x=n, linestyle='--', color='red')\n",
        "        plt.text(n, cumulativa[n-1], f'{int(p*100)}%', color='red')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return resultados\n",
        "\n",
        "resultados = analisar_distribuicao_palavras(contador)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5t32EXPa2h-"
      },
      "outputs": [],
      "source": [
        "def plotar_curva_com_corte(contador, corte_percentual=15, corte_palavras=35):\n",
        "    contagens_ordenadas = np.array(sorted(contador.values(), reverse=True))\n",
        "    soma_total = contagens_ordenadas.sum()\n",
        "    cumulativa = np.cumsum(contagens_ordenadas) / soma_total\n",
        "\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(range(1, len(cumulativa)+1), cumulativa, marker='.', color='blue')\n",
        "    plt.title('Curva de Frequência Cumulativa das Palavras')\n",
        "    plt.xlabel('Top N palavras')\n",
        "    plt.ylabel('Frequência acumulada')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Linha vertical no ponto de corte (35 palavras)\n",
        "    plt.axvline(x=corte_palavras, linestyle='--', color='red', label=f'Corte: {corte_palavras} palavras')\n",
        "\n",
        "    # Linha horizontal indicando o percentual acumulado (15%)\n",
        "    plt.axhline(y=corte_percentual/100, linestyle='--', color='green', label=f'{corte_percentual}% acumulado')\n",
        "\n",
        "    # Marcar ponto exato\n",
        "    plt.scatter(corte_palavras, cumulativa[corte_palavras-1], color='red', zorder=5)\n",
        "    plt.text(corte_palavras+20, cumulativa[corte_palavras-1]-0.02,\n",
        "             f'{corte_palavras} palavras\\n({corte_percentual}%)',\n",
        "             color='red')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plotar_curva_com_corte(contador, corte_percentual=20, corte_palavras=55)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKv5QUh8PS7-"
      },
      "source": [
        "## 4. Pré-Processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOmiAwPQEnmR"
      },
      "outputs": [],
      "source": [
        "# Executa pré-processamento\n",
        "stopwords_extras = carregar_palavras_mais_frequentes(df['ementa'], stopwords_pt, top_n = TOP_N_WORDS)[0]\n",
        "stopwords_pt.update(stopwords_extras)\n",
        "\n",
        "df['ementa_preprocessada'] = df['ementa'].apply(lematizar_texto)\n",
        "df['ementa_preprocessada'] = df['ementa_preprocessada'].apply(lambda x: remover_stopwords(x, stopwords_pt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmCfkGtpWh0O"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN74L0swQRfe"
      },
      "source": [
        "## 5. Análise Exploratória Depois do Pré-Processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGGgJOHsQY4H"
      },
      "source": [
        "### Recalcular estatísticas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uipuXlJdQUjk"
      },
      "outputs": [],
      "source": [
        "df['n_caracteres_preproc'] = df['ementa_preprocessada'].apply(contar_caracteres)\n",
        "df['n_palavras_preproc'] = df['ementa_preprocessada'].apply(contar_palavras)\n",
        "\n",
        "print(df[['n_palavras', 'n_palavras_preproc', 'n_caracteres', 'n_caracteres_preproc']].describe())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL9zNDTYQnvi"
      },
      "source": [
        "### Comparação antes e depois"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuPTQH_rQrQi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Estatísticas\n",
        "media_antes = df['n_palavras'].mean()\n",
        "mediana_antes = df['n_palavras'].median()\n",
        "\n",
        "media_depois = df['n_palavras_preproc'].mean()\n",
        "mediana_depois = df['n_palavras_preproc'].median()\n",
        "\n",
        "plt.figure(figsize=(12,7))\n",
        "\n",
        "# Histograma Antes\n",
        "sns.histplot(df['n_palavras'], bins=50, kde=True, color='royalblue', label='Antes do Pré-processamento', alpha=0.5)\n",
        "\n",
        "# Histograma Depois\n",
        "sns.histplot(df['n_palavras_preproc'], bins=50, kde=True, color='seagreen', label='Após o Pré-processamento', alpha=0.5)\n",
        "\n",
        "# Linhas de Média\n",
        "plt.axvline(media_antes, color='blue', linestyle='--', linewidth=2, label=f'Média Antes: {media_antes:.0f}')\n",
        "plt.axvline(media_depois, color='green', linestyle='--', linewidth=2, label=f'Média Depois: {media_depois:.0f}')\n",
        "\n",
        "# Linhas de Mediana\n",
        "plt.axvline(mediana_antes, color='blue', linestyle='-', linewidth=2, label=f'Mediana Antes: {mediana_antes:.0f}')\n",
        "plt.axvline(mediana_depois, color='green', linestyle='-', linewidth=2, label=f'Mediana Depois: {mediana_depois:.0f}')\n",
        "\n",
        "plt.title('Distribuição do Número de Palavras nas Ementas\\nAntes e Após o Pré-processamento', fontsize=14)\n",
        "plt.xlabel('Número de Palavras por Ementa')\n",
        "plt.ylabel('Frequência de Ementas')\n",
        "plt.legend()\n",
        "plt.grid(visible=True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI-oOPLxRBBo"
      },
      "source": [
        "### Nuvem de palavras após pré-processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YluvU8xRDmc"
      },
      "outputs": [],
      "source": [
        "# Junta cada lista de tokens em uma string\n",
        "text_all_clean = ' '.join([' '.join(ementa) for ementa in df['ementa_preprocessada']])\n",
        "\n",
        "# Gera a nuvem de palavras\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wordcloud_clean = WordCloud(\n",
        "    width=800,\n",
        "    height=400,\n",
        "    background_color='white',\n",
        "    colormap='plasma'\n",
        ").generate(text_all_clean)\n",
        "\n",
        "# Plota\n",
        "plt.figure(figsize=(15,7))\n",
        "plt.imshow(wordcloud_clean, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Nuvem de Palavras (Depois do Pré-processamento)', fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aZ_y0wcDA9_"
      },
      "source": [
        "## Execução do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rFnyOgQDD7S"
      },
      "outputs": [],
      "source": [
        "modelos = ['fasttext', 'doc2vec', 'sbert', 'bertimbau', 'legalbert']\n",
        "df_resultados, embeddings_dict, labels_dict, reduzidos_dict, k_dict, modelos_cluster = comparar_modelos(\n",
        "    modelos, df['ementa_preprocessada'], n_components=5,\n",
        "    min_cluster_size=30\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbTp3yuBxqb-"
      },
      "source": [
        "# Resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIxZkA9sxuOR"
      },
      "source": [
        "## Tabela de Comparação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sJ2XWt8GxyY-"
      },
      "outputs": [],
      "source": [
        "tabela = df_resultados.sort_values(by='Silhueta', ascending=False)\n",
        "\n",
        "tabela_formatada = tabela.style.background_gradient(cmap='Reds', subset=['Tempo Pipeline (s)']) \\\n",
        "    .background_gradient(cmap='Blues_r', subset=['Silhueta']) \\\n",
        "    .background_gradient(cmap='Purples', subset=['Calinski-Harabasz']) \\\n",
        "    .background_gradient(cmap='Greens', subset=['Coherence c_v']) \\\n",
        "    .format(precision=4)\n",
        "\n",
        "tabela_formatada\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTayBAbsx3b-"
      },
      "source": [
        "## Gráficos Comparativos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Xn1vOuq9x8u-"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.barplot(x=df_resultados.index, y='Silhueta', data=df_resultados.reset_index(), palette='viridis')\n",
        "plt.title('Comparação da Métrica de Silhueta por Embedding', fontsize=14)\n",
        "plt.ylabel('Coeficiente de Silhueta')\n",
        "plt.xlabel('Modelo')\n",
        "plt.grid(visible=True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.barplot(x=df_resultados.index, y='Calinski-Harabasz', data=df_resultados.reset_index(), palette='viridis')\n",
        "plt.title('Comparação da Métrica de Calinski-Harabasz por Embedding', fontsize=14)\n",
        "plt.ylabel('Calinski-Harabasz')\n",
        "plt.xlabel('Modelo')\n",
        "plt.grid(visible=True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dl84oWxyRjN"
      },
      "source": [
        "## Radar Chart (Comparativo Multimétrico)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "on9MGiDayXGT"
      },
      "outputs": [],
      "source": [
        "from math import pi\n",
        "\n",
        "def plot_radar(df):\n",
        "    categorias = list(df.columns)\n",
        "    N = len(categorias)\n",
        "\n",
        "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
        "    angles += angles[:1]  # Completa o círculo\n",
        "\n",
        "    plt.figure(figsize=(8,8))\n",
        "    ax = plt.subplot(111, polar=True)\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        valores = row.tolist()\n",
        "        valores += valores[:1]\n",
        "        ax.plot(angles, valores, linewidth=2, label=idx)\n",
        "        ax.fill(angles, valores, alpha=0.1)\n",
        "\n",
        "    plt.xticks(angles[:-1], categorias, color='grey', size=8)\n",
        "    ax.set_rlabel_position(30)\n",
        "    plt.title('Comparação Multimétrica dos Embeddings', size=14)\n",
        "    plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n",
        "    plt.show()\n",
        "\n",
        "# Seleciona as métricas que fazem sentido para radar (evita métricas com escalas muito discrepantes)\n",
        "df_radar = df_resultados[['Silhueta', 'Calinski-Harabasz']].copy()\n",
        "\n",
        "# Normaliza para radar (0-1)\n",
        "for col in df_radar.columns:\n",
        "    max_ = df_radar[col].max()\n",
        "    min_ = df_radar[col].min()\n",
        "    df_radar[col] = (df_radar[col] - min_) / (max_ - min_)\n",
        "\n",
        "plot_radar(df_radar)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXgzEJWDyY_z"
      },
      "source": [
        "## Gerar Plot dos Clusters para Cada Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V1rQr9Zgyd9s"
      },
      "outputs": [],
      "source": [
        "for modelo in modelos:\n",
        "    plotar_clusters(\n",
        "        reduzidos_dict[modelo],\n",
        "        labels_dict[modelo],\n",
        "        titulo=f'Clusters - {modelo.upper()} (K={k_dict[modelo]})'\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8qQliJFb5nF"
      },
      "source": [
        "## Visualizar termos dos clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LK6hGEH0b9Zv"
      },
      "outputs": [],
      "source": [
        "for modelo in modelos:\n",
        "    visualizar_clusters_termos(\n",
        "        textos=df['ementa_preprocessada'],\n",
        "        labels=labels_dict[modelo],\n",
        "        modelo_nome=modelo,\n",
        "        top_n=10,\n",
        "        n_amostra=5\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}