{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoxxqC_FDzwN"
      },
      "source": [
        "# Prepara√ß√£o do Ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5o_BZ4a00R_"
      },
      "source": [
        "## Instala√ß√£o dos pacotes necess√°rios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCsn7P3twY1W"
      },
      "outputs": [],
      "source": [
        "!pip install nltk gensim spacy sentence-transformers scikit-learn seaborn matplotlib pandas hdbscan\n",
        "!python -m spacy download pt_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fkb9Yt_1BCb"
      },
      "source": [
        "## Importa√ß√£o das bibliotecas\n",
        "Ap√≥s a instala√ß√£o, tem que reiniciar a sess√£o."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BpQN56kzj6L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import zipfile\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from IPython.display import display\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "\n",
        "from gensim.models import FastText, Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import CoherenceModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import umap\n",
        "\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "from scipy.spatial.distance import cdist\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-VoOdWvA1as"
      },
      "source": [
        "# Defini√ß√£o de Fun√ß√µes √öteis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahml3XdLExDi"
      },
      "source": [
        "## Carregar corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxz97r65E0-x"
      },
      "outputs": [],
      "source": [
        "def carregar_corpus(path, qtde_max = 999999999):\n",
        "    ids = []\n",
        "    ementas = []\n",
        "\n",
        "    arquivos = [f for f in os.listdir(path) if f.endswith('.txt')]\n",
        "\n",
        "    cont = 0\n",
        "    for arquivo in arquivos:\n",
        "        if cont == qtde_max:\n",
        "            break\n",
        "        cont += 1\n",
        "\n",
        "        id_acordao = os.path.splitext(arquivo)[0]  # remove .txt\n",
        "        with open(os.path.join(path, arquivo), 'r', encoding='utf-8') as f:\n",
        "            texto = f.read().strip()\n",
        "\n",
        "        # Remove a cita√ß√£o final entre par√™nteses, se houver\n",
        "        texto_limpo = re.sub(r'\\s*\\(TRT.*\\)$', '', texto)\n",
        "\n",
        "        ids.append(id_acordao)\n",
        "        ementas.append(texto_limpo)\n",
        "\n",
        "    df = pd.DataFrame({'id_acordao': ids, 'ementa': ementas})\n",
        "    df['ementa'] = df['ementa'].astype(str)\n",
        "\n",
        "    print(f\"Total de ementas carregadas: {len(df)}\")\n",
        "    print(df.sample(5))\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GxdeukVmpCE"
      },
      "source": [
        "## Contar palavras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4QpPjHxmoIu"
      },
      "outputs": [],
      "source": [
        "def contar_palavras(texto):\n",
        "    if isinstance(texto, list):\n",
        "        return len(texto)\n",
        "    elif isinstance(texto, str):\n",
        "        return len(texto.split())\n",
        "    else:\n",
        "        return 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T9NiOs3-4Ag"
      },
      "source": [
        "## Contar caracteres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMHW-Jqp-6OR"
      },
      "outputs": [],
      "source": [
        "def contar_caracteres(texto):\n",
        "    if isinstance(texto, list):\n",
        "        return sum(len(p) for p in texto)\n",
        "    elif isinstance(texto, str):\n",
        "        return len(texto)\n",
        "    else:\n",
        "        return 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCVI0eNCYLVW"
      },
      "source": [
        "## Lematizar texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2f-HUIcYNw8"
      },
      "outputs": [],
      "source": [
        "def lematizar_texto(texto):\n",
        "    doc = nlp_spacy(texto)\n",
        "    lemas = [\n",
        "        token.lemma_\n",
        "        for token in doc\n",
        "        if token.is_alpha and not token.is_stop\n",
        "    ]\n",
        "    return lemas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLAI3KNNBh8M"
      },
      "source": [
        "## Remover stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjdyFDSWBlc4"
      },
      "outputs": [],
      "source": [
        "def remover_stopwords(text, stopwords_remover):\n",
        "\n",
        "    if isinstance(text, list):\n",
        "        text = ' '.join(text)  # transforma lista em string se for o caso\n",
        "\n",
        "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
        "    text = re.sub(r\"\\d+\", \" \", text)\n",
        "    tokens = word_tokenize(text.lower(), language=\"portuguese\")\n",
        "\n",
        "    tokens = [\n",
        "        token\n",
        "        for token in tokens\n",
        "        if token not in stopwords_remover and len(token) > 2 and token.isalpha()\n",
        "    ]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpHLNUMeDOZK"
      },
      "source": [
        "## Carregar palavras mais frequentes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmq54irhYeAg"
      },
      "outputs": [],
      "source": [
        "def carregar_palavras_mais_frequentes(lista_ementa, stopwords_ignorar, top_n=20):\n",
        "\n",
        "    # Junta todas as ementas em um √∫nico texto\n",
        "    texto_unificado = ' '.join([str(e) for e in lista_ementa])\n",
        "\n",
        "    # Limpeza b√°sica\n",
        "    texto_unificado = re.sub(r\"[^\\w\\s]\", \" \", texto_unificado)  # remove pontua√ß√£o\n",
        "    texto_unificado = re.sub(r\"\\d+\", \" \", texto_unificado)      # remove n√∫meros\n",
        "    texto_unificado = texto_unificado.lower()                   # min√∫sculas\n",
        "\n",
        "    # Tokeniza√ß√£o\n",
        "    tokens = word_tokenize(texto_unificado, language='portuguese')\n",
        "\n",
        "    # Remove stopwords e tokens n√£o alfab√©ticos\n",
        "    tokens_filtrados = [\n",
        "        token for token in tokens\n",
        "        if token not in stopwords_ignorar and len(token) > 2 and token.isalpha()\n",
        "    ]\n",
        "\n",
        "    # Contagem das palavras\n",
        "    counter = Counter(tokens_filtrados)\n",
        "    most_common = counter.most_common(top_n)\n",
        "\n",
        "    # Separa palavras e frequ√™ncias\n",
        "    palavras, contagens = zip(*most_common) if most_common else ([], [])\n",
        "\n",
        "    return palavras, contagens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTDbTXlEBZSI"
      },
      "source": [
        "## Gerar Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcNNQ0ZtBb1j"
      },
      "outputs": [],
      "source": [
        "def gerar_embeddings(corpus, modelo='sbert'):\n",
        "\n",
        "    corpus_texto = [' '.join(doc) if isinstance(doc, list) else doc for doc in corpus]\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    if modelo == 'fasttext':\n",
        "        tokenizado = [doc.split() for doc in corpus_texto]\n",
        "        ft_model = FastText(sentences=tokenizado, vector_size=300, window=5, min_count=2, workers=4, epochs=20)\n",
        "        for doc in tokenizado:\n",
        "            vetor = np.mean(\n",
        "                [ft_model.wv[word] for word in doc if word in ft_model.wv] or [np.zeros(300)],\n",
        "                axis=0\n",
        "            )\n",
        "            embeddings.append(vetor)\n",
        "\n",
        "    elif modelo == 'doc2vec':\n",
        "        tokenizado = [doc.split() for doc in corpus_texto]\n",
        "        documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokenizado)]\n",
        "        d2v_model = Doc2Vec(documents, vector_size=300, window=5, min_count=2, workers=4, epochs=40)\n",
        "        for doc in tokenizado:\n",
        "            if len(doc) == 0:\n",
        "                vetor = np.zeros(300)\n",
        "            else:\n",
        "                vetor = d2v_model.infer_vector(doc)\n",
        "            embeddings.append(vetor)\n",
        "\n",
        "    elif modelo == 'sbert':\n",
        "        sbert = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "        embeddings = sbert.encode(corpus_texto, show_progress_bar=True)\n",
        "\n",
        "    elif modelo == 'bertimbau':\n",
        "        sbert = SentenceTransformer('neuralmind/bert-base-portuguese-cased')\n",
        "        embeddings = sbert.encode(corpus_texto, show_progress_bar=True)\n",
        "\n",
        "    elif modelo == 'legalbert':\n",
        "        sbert = SentenceTransformer('raquelsilveira/legalbertpt_fp')\n",
        "        embeddings = sbert.encode(corpus_texto, show_progress_bar=True)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Modelo n√£o reconhecido. Escolha entre 'fasttext', 'doc2vec', 'sbert', 'bertimbau' ou 'legalbert'.\")\n",
        "\n",
        "    embeddings = np.vstack(embeddings) if isinstance(embeddings, list) else np.array(embeddings)\n",
        "    return embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPKWGGF4kEzj"
      },
      "source": [
        "## Reduzir dimensionalidade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yleCdcqkH2d"
      },
      "outputs": [],
      "source": [
        "def reduzir_dimensionalidade(embeddings, n_components=2):\n",
        "    reducer = umap.UMAP(n_components=n_components, random_state=SEED)\n",
        "    reduzido = reducer.fit_transform(embeddings)\n",
        "    return reduzido\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBSqnwUukx7B"
      },
      "source": [
        "## Aplicar clusteriza√ß√£o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgTxgroNk0W_"
      },
      "outputs": [],
      "source": [
        "def aplicar_clusterizacao(embeddings, min_cluster_size=5):\n",
        "    modelo = HDBSCAN(\n",
        "        min_cluster_size=min_cluster_size,\n",
        "        metric='euclidean',\n",
        "        cluster_selection_method='eom'\n",
        "    ).fit(embeddings)\n",
        "    labels = modelo.labels_\n",
        "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "    print(f\"üî∏ HDBSCAN encontrou {n_clusters} clusters (sem contar ru√≠do)\")\n",
        "\n",
        "    return labels, modelo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmY53e6Xk6sM"
      },
      "source": [
        "## Extrair termos relevantes cluster (tf-idf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GHJ3NQ3k--t"
      },
      "outputs": [],
      "source": [
        "def termos_por_cluster(textos, labels, top_n=10):\n",
        "\n",
        "    textos_str = [' '.join(doc) if isinstance(doc, list) else doc for doc in textos]\n",
        "\n",
        "    df = pd.DataFrame({'texto': textos_str, 'cluster': labels})\n",
        "\n",
        "    resultados = {}\n",
        "    for cluster in sorted(df['cluster'].unique()):\n",
        "        docs = df[df['cluster'] == cluster]['texto']\n",
        "        vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "        X = vectorizer.fit_transform(docs)\n",
        "        indices = np.argsort(X.toarray().sum(axis=0))[::-1]\n",
        "        termos = [vectorizer.get_feature_names_out()[i] for i in indices[:top_n]]\n",
        "        resultados[cluster] = termos\n",
        "\n",
        "    return resultados\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stYxZS4tlGUh"
      },
      "source": [
        "## Avaliar clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQtI6dWAlKzn"
      },
      "outputs": [],
      "source": [
        "def avaliar_clusters(embeddings, labels, modelo):\n",
        "    mask = labels != -1  # Ignorar ru√≠do para HDBSCAN\n",
        "\n",
        "    if len(set(labels[mask])) <= 1:\n",
        "        print(\"‚ö†Ô∏è Apenas um cluster detectado (ou ru√≠do excessivo). Avalia√ß√£o n√£o aplic√°vel.\")\n",
        "        return {\n",
        "            'Silhueta': np.nan,\n",
        "            'Calinski-Harabasz': np.nan\n",
        "        }\n",
        "\n",
        "    resultados = {}\n",
        "\n",
        "    resultados['Silhueta'] = silhouette_score(embeddings[mask], labels[mask])\n",
        "    resultados['Calinski-Harabasz'] = calinski_harabasz_score(embeddings[mask], labels[mask])\n",
        "\n",
        "    return resultados\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jakZCMHBhqL"
      },
      "source": [
        "## Calcular coherence score (c_v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOVAgPzfBlko"
      },
      "outputs": [],
      "source": [
        "def calcular_coherence(textos, labels, top_n=10):\n",
        "\n",
        "    df = pd.DataFrame({'texto': textos, 'cluster': labels})\n",
        "    df = df[df['cluster'] != -1].reset_index(drop=True)  # Remove ru√≠dos e reseta os √≠ndices\n",
        "\n",
        "    # Vetorizar todos os documentos\n",
        "    docs_str = df['texto'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "    tfidf = vectorizer.fit_transform(docs_str)\n",
        "    termos = vectorizer.get_feature_names_out()\n",
        "\n",
        "    topics = []\n",
        "\n",
        "    for cluster in sorted(df['cluster'].unique()):\n",
        "        docs_cluster = df[df['cluster'] == cluster]\n",
        "        indices = docs_cluster.index\n",
        "        submatriz = tfidf[indices]\n",
        "\n",
        "        if submatriz.shape[0] == 0:\n",
        "            continue\n",
        "\n",
        "        # Soma TF-IDF dos termos no cluster e seleciona os top_n\n",
        "        tfidf_soma = np.asarray(submatriz.sum(axis=0)).flatten()\n",
        "        top_indices = tfidf_soma.argsort()[-top_n:][::-1]\n",
        "        top_termos = [termos[i] for i in top_indices]\n",
        "        topics.append(top_termos)\n",
        "\n",
        "    # Preparar corpus e dictionary para CoherenceModel\n",
        "    all_tokens = [doc.split() if isinstance(doc, str) else doc for doc in textos]\n",
        "    dictionary = Dictionary(all_tokens)\n",
        "\n",
        "    coherence_model = CoherenceModel(\n",
        "        topics=topics,\n",
        "        texts=all_tokens,\n",
        "        dictionary=dictionary,\n",
        "        coherence='c_v'\n",
        "    )\n",
        "\n",
        "    coherence = coherence_model.get_coherence()\n",
        "    print(f\"üî∏ Coherence Score (TF-IDF): {coherence:.4f}\")\n",
        "    return coherence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GukAAxb2lVim"
      },
      "source": [
        "## Plotar clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHvk1WH5lXe3"
      },
      "outputs": [],
      "source": [
        "def plotar_clusters(embeddings_reduzidos, labels, titulo='Clusters'):\n",
        "\n",
        "    plt.figure(figsize=(10,7))\n",
        "    palette = sns.color_palette(\"tab10\", len(set(labels)))\n",
        "\n",
        "    sns.scatterplot(\n",
        "        x=embeddings_reduzidos[:, 0],\n",
        "        y=embeddings_reduzidos[:, 1],\n",
        "        hue=labels,\n",
        "        palette=palette,\n",
        "        legend='full',\n",
        "        alpha=0.7,\n",
        "        s=60,\n",
        "        edgecolor='black'\n",
        "    )\n",
        "\n",
        "    plt.title(titulo, fontsize=14)\n",
        "    plt.xlabel('Componente 1')\n",
        "    plt.ylabel('Componente 2')\n",
        "    plt.grid(visible=True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "    plt.legend(title='Cluster', loc='best')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riNJH2jqwmrc"
      },
      "source": [
        "## Pipeline avalia√ß√£o embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UngReryUwtjq"
      },
      "outputs": [],
      "source": [
        "def pipeline_embedding(\n",
        "    nome_modelo, textos, n_components=2,\n",
        "    min_cluster_size=5\n",
        "):\n",
        "\n",
        "    print(f'\\nüîπ Processando modelo: {nome_modelo}')\n",
        "\n",
        "    inicio = time.time()\n",
        "    embeddings = gerar_embeddings(textos, modelo=nome_modelo)\n",
        "\n",
        "    reduzido = reduzir_dimensionalidade(embeddings, n_components=n_components)\n",
        "\n",
        "    melhor_k = None\n",
        "\n",
        "    labels, modelo_cluster = aplicar_clusterizacao(\n",
        "        embeddings, min_cluster_size=min_cluster_size\n",
        "    )\n",
        "\n",
        "    tempo_embedding = time.time() - inicio\n",
        "    print(f'‚è≥ Tempo Pipeline ({nome_modelo}): {tempo_embedding:.2f}s')\n",
        "\n",
        "    avaliacao = avaliar_clusters(embeddings, labels, modelo_cluster)\n",
        "    avaliacao['Modelo'] = nome_modelo\n",
        "    avaliacao['K'] = (len(set(labels)) - (1 if -1 in labels else 0))\n",
        "    avaliacao['Tempo Pipeline (s)'] = tempo_embedding\n",
        "    avaliacao['Coherence c_v'] = calcular_coherence(textos, labels)\n",
        "\n",
        "    return avaliacao, labels, embeddings, reduzido, avaliacao['K'], modelo_cluster\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni43QuG7w8LI"
      },
      "source": [
        "## Comparar modelos e coletar avalia√ß√µes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW68Y3fcxBDb"
      },
      "outputs": [],
      "source": [
        "def comparar_modelos(\n",
        "    modelos, textos, n_components=2,\n",
        "    min_cluster_size=5\n",
        "):\n",
        "    resultados = []\n",
        "    embeddings_dict = {}\n",
        "    labels_dict = {}\n",
        "    reduzidos_dict = {}\n",
        "    k_dict = {}\n",
        "    modelos_cluster = {}\n",
        "\n",
        "    for modelo in modelos:\n",
        "        avaliacao, labels, embeddings, reduzido, k, modelo_cluster = pipeline_embedding(\n",
        "            modelo, textos, min_cluster_size=min_cluster_size,\n",
        "            n_components=n_components\n",
        "        )\n",
        "        resultados.append(avaliacao)\n",
        "        embeddings_dict[modelo] = embeddings\n",
        "        labels_dict[modelo] = labels\n",
        "        reduzidos_dict[modelo] = reduzido\n",
        "        k_dict[modelo] = k\n",
        "        modelos_cluster[modelo] = modelo_cluster\n",
        "\n",
        "    df_resultados = pd.DataFrame(resultados)\n",
        "    df_resultados.set_index('Modelo', inplace=True)\n",
        "\n",
        "    return df_resultados, embeddings_dict, labels_dict, reduzidos_dict, k_dict, modelos_cluster\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFtovvL_bwFn"
      },
      "source": [
        "## Visualizar termos do cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ByxTImdbzA8"
      },
      "outputs": [],
      "source": [
        "def visualizar_clusters_termos(textos, labels, modelo_nome, top_n=10, n_amostra=5):\n",
        "    df = pd.DataFrame({'texto': textos, 'cluster': labels})\n",
        "    clusters_unicos = [c for c in set(labels) if c != -1]\n",
        "    clusters_amostrados = np.random.choice(clusters_unicos, min(n_amostra, len(clusters_unicos)), replace=False)\n",
        "\n",
        "    print(f'\\nüü¶ Modelo: {modelo_nome}')\n",
        "    print(f'Clusters amostrados: {clusters_amostrados}')\n",
        "\n",
        "    for cluster in clusters_amostrados:\n",
        "        docs = df[df['cluster'] == cluster]['texto']\n",
        "        docs_tokens = [doc if isinstance(doc, list) else doc.split() for doc in docs]\n",
        "        all_tokens = [token for doc in docs_tokens for token in doc]\n",
        "        contador = Counter(all_tokens)\n",
        "        termos_top = [t for t, _ in contador.most_common(top_n)]\n",
        "\n",
        "        print(f'\\nüîπ Cluster {cluster}:')\n",
        "        print(f'Termos mais representativos: {termos_top}')\n",
        "        print(f'N√∫mero de documentos no cluster: {len(docs)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDe8-Dd2-3fZ"
      },
      "source": [
        "# Execu√ß√£o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjuJkPAJ_B-a"
      },
      "source": [
        "## Define vari√°veis e par√¢metros gerais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDOONzSv_FrE"
      },
      "outputs": [],
      "source": [
        "# Sementes globais\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "QTDE_ACORDAOS_PROCESSADOS = 10000\n",
        "\n",
        "# palavras mais comuns\n",
        "TOP_N_WORDS = 55\n",
        "\n",
        "# Caminho do arquivo\n",
        "path = '/content/ementas'\n",
        "df = carregar_corpus(path, QTDE_ACORDAOS_PROCESSADOS)\n",
        "\n",
        "# Stopwords\n",
        "stopwords_pt = set(stopwords.words(\"portuguese\"))\n",
        "\n",
        "# Carrega o modelo de portugu√™s\n",
        "nlp_spacy = spacy.load('pt_core_news_sm')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvCtH_h-OTNS"
      },
      "source": [
        "## 3. An√°lise Explorat√≥ria Antes do Pr√©-Processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdBPb9TO8NW9"
      },
      "source": [
        "### Amostra do corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz3zu5jh7_PP"
      },
      "outputs": [],
      "source": [
        "# Adiciona colunas de contagem\n",
        "df['qtde_palavras'] = df['ementa'].apply(contar_palavras)\n",
        "df['qtde_caractere'] = df['ementa'].apply(lambda x: len(str(x)))\n",
        "\n",
        "# Gera amostra com 5 ementas\n",
        "amostra = df[['id_acordao', 'ementa', 'qtde_palavras', 'qtde_caractere']].sample(5)\n",
        "\n",
        "display(amostra)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnGz7g_IPCAs"
      },
      "source": [
        "### Nuvem de palavras (sem pr√©-processamento)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_OU_KJgPDjv"
      },
      "outputs": [],
      "source": [
        "text_all = ' '.join(df['ementa'])\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text_all)\n",
        "\n",
        "plt.figure(figsize=(15,7))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Nuvem de Palavras (Antes do Pr√©-processamento)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qso-F6ZOa4E"
      },
      "source": [
        "### Tamanho dos textos (em caracteres e palavras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_0_giZbOSji"
      },
      "outputs": [],
      "source": [
        "df['n_caracteres'] = df['ementa'].apply(len)\n",
        "df['n_palavras'] = df['ementa'].apply(contar_palavras)\n",
        "\n",
        "print(df[['n_caracteres', 'n_palavras']].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6gonnhYOnPR"
      },
      "source": [
        "### Histograma do tamanho dos textos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Siv1QKYJOq-s"
      },
      "outputs": [],
      "source": [
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Calcula estat√≠sticas\n",
        "media = df['n_palavras'].mean()\n",
        "mediana = df['n_palavras'].median()\n",
        "desvio = df['n_palavras'].std()\n",
        "\n",
        "plt.figure(figsize=(14,7))\n",
        "sns.histplot(df['n_palavras'], bins=50, kde=True, color='royalblue')\n",
        "\n",
        "# Linhas de m√©dia e mediana\n",
        "plt.axvline(media, color='red', linestyle='--', linewidth=2, label=f'M√©dia: {media:.0f}')\n",
        "plt.axvline(mediana, color='green', linestyle='-', linewidth=2, label=f'Mediana: {mediana:.0f}')\n",
        "\n",
        "plt.title('Distribui√ß√£o do N√∫mero de Palavras nas Ementas\\n(Antes do Pr√©-processamento)', fontsize=16)\n",
        "plt.xlabel('N√∫mero de Palavras por Ementa', fontsize=12)\n",
        "plt.ylabel('Frequ√™ncia de Ementas', fontsize=12)\n",
        "\n",
        "plt.grid(visible=True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhikN8ZkRd1W"
      },
      "source": [
        "### Frequ√™ncia das Palavras Mais Comuns (stopwords customizadas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRcmgtuMRghW"
      },
      "outputs": [],
      "source": [
        "df['ementa_lematizada'] = df['ementa'].apply(lematizar_texto)\n",
        "\n",
        "palavras, contagens = carregar_palavras_mais_frequentes(df['ementa_lematizada'], stopwords_pt, top_n = TOP_N_WORDS)\n",
        "plt.figure(figsize=(12,12))\n",
        "sns.barplot(x=list(contagens), y=list(palavras), palette='viridis')\n",
        "plt.title(f'Top {TOP_N_WORDS} Palavras Mais Frequentes (Ap√≥s Pr√©-processamento)')\n",
        "plt.xlabel('Frequ√™ncia')\n",
        "plt.ylabel('Palavras')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMJwFIKBUyIV"
      },
      "outputs": [],
      "source": [
        "def analisar_distribuicao_palavras(contador):\n",
        "    contagens_ordenadas = np.array(sorted(contador.values(), reverse=True))\n",
        "    soma_total = contagens_ordenadas.sum()\n",
        "    cumulativa = np.cumsum(contagens_ordenadas) / soma_total\n",
        "\n",
        "    pontos = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99]\n",
        "    resultados = {}\n",
        "\n",
        "    for p in pontos:\n",
        "        idx = np.searchsorted(cumulativa, p)\n",
        "        resultados[p] = idx + 1  # +1 porque index come√ßa no zero\n",
        "\n",
        "    print(\"Palavras necess√°rias para atingir os percentuais acumulados:\")\n",
        "    for p, n in resultados.items():\n",
        "        print(f\"{int(p*100)}% -> {n} palavras\")\n",
        "\n",
        "    # Plotando a curva\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(range(1, len(cumulativa)+1), cumulativa, marker='.')\n",
        "    plt.title('Curva de Frequ√™ncia Cumulativa')\n",
        "    plt.xlabel('Top N palavras')\n",
        "    plt.ylabel('Frequ√™ncia acumulada')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Marcar os pontos de interesse\n",
        "    for p, n in resultados.items():\n",
        "        plt.axvline(x=n, linestyle='--', color='red')\n",
        "        plt.text(n, cumulativa[n-1], f'{int(p*100)}%', color='red')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return resultados\n",
        "\n",
        "resultados = analisar_distribuicao_palavras(contador)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5t32EXPa2h-"
      },
      "outputs": [],
      "source": [
        "def plotar_curva_com_corte(contador, corte_percentual=15, corte_palavras=35):\n",
        "    contagens_ordenadas = np.array(sorted(contador.values(), reverse=True))\n",
        "    soma_total = contagens_ordenadas.sum()\n",
        "    cumulativa = np.cumsum(contagens_ordenadas) / soma_total\n",
        "\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(range(1, len(cumulativa)+1), cumulativa, marker='.', color='blue')\n",
        "    plt.title('Curva de Frequ√™ncia Cumulativa das Palavras')\n",
        "    plt.xlabel('Top N palavras')\n",
        "    plt.ylabel('Frequ√™ncia acumulada')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Linha vertical no ponto de corte (35 palavras)\n",
        "    plt.axvline(x=corte_palavras, linestyle='--', color='red', label=f'Corte: {corte_palavras} palavras')\n",
        "\n",
        "    # Linha horizontal indicando o percentual acumulado (15%)\n",
        "    plt.axhline(y=corte_percentual/100, linestyle='--', color='green', label=f'{corte_percentual}% acumulado')\n",
        "\n",
        "    # Marcar ponto exato\n",
        "    plt.scatter(corte_palavras, cumulativa[corte_palavras-1], color='red', zorder=5)\n",
        "    plt.text(corte_palavras+20, cumulativa[corte_palavras-1]-0.02,\n",
        "             f'{corte_palavras} palavras\\n({corte_percentual}%)',\n",
        "             color='red')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plotar_curva_com_corte(contador, corte_percentual=20, corte_palavras=55)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKv5QUh8PS7-"
      },
      "source": [
        "## 4. Pr√©-Processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOmiAwPQEnmR"
      },
      "outputs": [],
      "source": [
        "# Executa pr√©-processamento\n",
        "stopwords_extras = carregar_palavras_mais_frequentes(df['ementa'], stopwords_pt, top_n = TOP_N_WORDS)[0]\n",
        "stopwords_pt.update(stopwords_extras)\n",
        "\n",
        "df['ementa_preprocessada'] = df['ementa'].apply(lematizar_texto)\n",
        "df['ementa_preprocessada'] = df['ementa_preprocessada'].apply(lambda x: remover_stopwords(x, stopwords_pt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmCfkGtpWh0O"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN74L0swQRfe"
      },
      "source": [
        "## 5. An√°lise Explorat√≥ria Depois do Pr√©-Processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGGgJOHsQY4H"
      },
      "source": [
        "### Recalcular estat√≠sticas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uipuXlJdQUjk"
      },
      "outputs": [],
      "source": [
        "df['n_caracteres_preproc'] = df['ementa_preprocessada'].apply(contar_caracteres)\n",
        "df['n_palavras_preproc'] = df['ementa_preprocessada'].apply(contar_palavras)\n",
        "\n",
        "print(df[['n_palavras', 'n_palavras_preproc', 'n_caracteres', 'n_caracteres_preproc']].describe())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL9zNDTYQnvi"
      },
      "source": [
        "### Compara√ß√£o antes e depois"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuPTQH_rQrQi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Estat√≠sticas\n",
        "media_antes = df['n_palavras'].mean()\n",
        "mediana_antes = df['n_palavras'].median()\n",
        "\n",
        "media_depois = df['n_palavras_preproc'].mean()\n",
        "mediana_depois = df['n_palavras_preproc'].median()\n",
        "\n",
        "plt.figure(figsize=(12,7))\n",
        "\n",
        "# Histograma Antes\n",
        "sns.histplot(df['n_palavras'], bins=50, kde=True, color='royalblue', label='Antes do Pr√©-processamento', alpha=0.5)\n",
        "\n",
        "# Histograma Depois\n",
        "sns.histplot(df['n_palavras_preproc'], bins=50, kde=True, color='seagreen', label='Ap√≥s o Pr√©-processamento', alpha=0.5)\n",
        "\n",
        "# Linhas de M√©dia\n",
        "plt.axvline(media_antes, color='blue', linestyle='--', linewidth=2, label=f'M√©dia Antes: {media_antes:.0f}')\n",
        "plt.axvline(media_depois, color='green', linestyle='--', linewidth=2, label=f'M√©dia Depois: {media_depois:.0f}')\n",
        "\n",
        "# Linhas de Mediana\n",
        "plt.axvline(mediana_antes, color='blue', linestyle='-', linewidth=2, label=f'Mediana Antes: {mediana_antes:.0f}')\n",
        "plt.axvline(mediana_depois, color='green', linestyle='-', linewidth=2, label=f'Mediana Depois: {mediana_depois:.0f}')\n",
        "\n",
        "plt.title('Distribui√ß√£o do N√∫mero de Palavras nas Ementas\\nAntes e Ap√≥s o Pr√©-processamento', fontsize=14)\n",
        "plt.xlabel('N√∫mero de Palavras por Ementa')\n",
        "plt.ylabel('Frequ√™ncia de Ementas')\n",
        "plt.legend()\n",
        "plt.grid(visible=True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI-oOPLxRBBo"
      },
      "source": [
        "### Nuvem de palavras ap√≥s pr√©-processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YluvU8xRDmc"
      },
      "outputs": [],
      "source": [
        "# Junta cada lista de tokens em uma string\n",
        "text_all_clean = ' '.join([' '.join(ementa) for ementa in df['ementa_preprocessada']])\n",
        "\n",
        "# Gera a nuvem de palavras\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wordcloud_clean = WordCloud(\n",
        "    width=800,\n",
        "    height=400,\n",
        "    background_color='white',\n",
        "    colormap='plasma'\n",
        ").generate(text_all_clean)\n",
        "\n",
        "# Plota\n",
        "plt.figure(figsize=(15,7))\n",
        "plt.imshow(wordcloud_clean, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Nuvem de Palavras (Depois do Pr√©-processamento)', fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aZ_y0wcDA9_"
      },
      "source": [
        "## Execu√ß√£o do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rFnyOgQDD7S"
      },
      "outputs": [],
      "source": [
        "modelos = ['fasttext', 'doc2vec', 'sbert', 'bertimbau', 'legalbert']\n",
        "df_resultados, embeddings_dict, labels_dict, reduzidos_dict, k_dict, modelos_cluster = comparar_modelos(\n",
        "    modelos, df['ementa_preprocessada'], n_components=5,\n",
        "    min_cluster_size=30\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbTp3yuBxqb-"
      },
      "source": [
        "# Resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIxZkA9sxuOR"
      },
      "source": [
        "## Tabela de Compara√ß√£o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sJ2XWt8GxyY-"
      },
      "outputs": [],
      "source": [
        "tabela = df_resultados.sort_values(by='Silhueta', ascending=False)\n",
        "\n",
        "tabela_formatada = tabela.style.background_gradient(cmap='Reds', subset=['Tempo Pipeline (s)']) \\\n",
        "    .background_gradient(cmap='Blues_r', subset=['Silhueta']) \\\n",
        "    .background_gradient(cmap='Purples', subset=['Calinski-Harabasz']) \\\n",
        "    .background_gradient(cmap='Greens', subset=['Coherence c_v']) \\\n",
        "    .format(precision=4)\n",
        "\n",
        "tabela_formatada\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTayBAbsx3b-"
      },
      "source": [
        "## Gr√°ficos Comparativos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Xn1vOuq9x8u-"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.barplot(x=df_resultados.index, y='Silhueta', data=df_resultados.reset_index(), palette='viridis')\n",
        "plt.title('Compara√ß√£o da M√©trica de Silhueta por Embedding', fontsize=14)\n",
        "plt.ylabel('Coeficiente de Silhueta')\n",
        "plt.xlabel('Modelo')\n",
        "plt.grid(visible=True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.barplot(x=df_resultados.index, y='Calinski-Harabasz', data=df_resultados.reset_index(), palette='viridis')\n",
        "plt.title('Compara√ß√£o da M√©trica de Calinski-Harabasz por Embedding', fontsize=14)\n",
        "plt.ylabel('Calinski-Harabasz')\n",
        "plt.xlabel('Modelo')\n",
        "plt.grid(visible=True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dl84oWxyRjN"
      },
      "source": [
        "## Radar Chart (Comparativo Multim√©trico)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "on9MGiDayXGT"
      },
      "outputs": [],
      "source": [
        "from math import pi\n",
        "\n",
        "def plot_radar(df):\n",
        "    categorias = list(df.columns)\n",
        "    N = len(categorias)\n",
        "\n",
        "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
        "    angles += angles[:1]  # Completa o c√≠rculo\n",
        "\n",
        "    plt.figure(figsize=(8,8))\n",
        "    ax = plt.subplot(111, polar=True)\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        valores = row.tolist()\n",
        "        valores += valores[:1]\n",
        "        ax.plot(angles, valores, linewidth=2, label=idx)\n",
        "        ax.fill(angles, valores, alpha=0.1)\n",
        "\n",
        "    plt.xticks(angles[:-1], categorias, color='grey', size=8)\n",
        "    ax.set_rlabel_position(30)\n",
        "    plt.title('Compara√ß√£o Multim√©trica dos Embeddings', size=14)\n",
        "    plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n",
        "    plt.show()\n",
        "\n",
        "# Seleciona as m√©tricas que fazem sentido para radar (evita m√©tricas com escalas muito discrepantes)\n",
        "df_radar = df_resultados[['Silhueta', 'Calinski-Harabasz']].copy()\n",
        "\n",
        "# Normaliza para radar (0-1)\n",
        "for col in df_radar.columns:\n",
        "    max_ = df_radar[col].max()\n",
        "    min_ = df_radar[col].min()\n",
        "    df_radar[col] = (df_radar[col] - min_) / (max_ - min_)\n",
        "\n",
        "plot_radar(df_radar)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXgzEJWDyY_z"
      },
      "source": [
        "## Gerar Plot dos Clusters para Cada Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V1rQr9Zgyd9s"
      },
      "outputs": [],
      "source": [
        "for modelo in modelos:\n",
        "    plotar_clusters(\n",
        "        reduzidos_dict[modelo],\n",
        "        labels_dict[modelo],\n",
        "        titulo=f'Clusters - {modelo.upper()} (K={k_dict[modelo]})'\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8qQliJFb5nF"
      },
      "source": [
        "## Visualizar termos dos clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LK6hGEH0b9Zv"
      },
      "outputs": [],
      "source": [
        "for modelo in modelos:\n",
        "    visualizar_clusters_termos(\n",
        "        textos=df['ementa_preprocessada'],\n",
        "        labels=labels_dict[modelo],\n",
        "        modelo_nome=modelo,\n",
        "        top_n=10,\n",
        "        n_amostra=5\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}